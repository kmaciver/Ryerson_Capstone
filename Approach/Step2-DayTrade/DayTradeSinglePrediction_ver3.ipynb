{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "DayTradeSinglePrediction_ver3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmaciver/Ryerson_Capstone/blob/master/Approach/Step2-DayTrade/DayTradeSinglePrediction_ver3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucrs5sJ-mh5G",
        "colab_type": "code",
        "outputId": "170c7160-7e8e-4b8d-981c-2f737e5daabc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QB9qARYlsLR",
        "colab_type": "code",
        "outputId": "1812858a-6c61-4323-a289-b423905d8270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms9FlgRKlsLg",
        "colab_type": "code",
        "outputId": "5f98333a-d65e-4af6-c8a8-dbc4c7360a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, LSTM, TimeDistributed, Lambda\n",
        "from tensorflow.python.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from keras import losses"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLY_ntS2lsLp",
        "colab_type": "code",
        "outputId": "d8cbdcd7-02b3-40aa-a3c5-2a9b4396ef9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "file_path = \"/content/drive/My Drive/Capstone/Data Exploration/Day_trade_data.csv\"\n",
        "DayTrade = pd.read_csv(file_path, index_col='Time')\n",
        "DayTrade = DayTrade.drop([DayTrade.columns[0]] ,  axis='columns')\n",
        "DayTrade.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_.BTC.</th>\n",
              "      <th>Volume_.Currency.</th>\n",
              "      <th>Weighted_Price</th>\n",
              "      <th>date</th>\n",
              "      <th>Open_RoC</th>\n",
              "      <th>High_RoC</th>\n",
              "      <th>Low_RoC</th>\n",
              "      <th>Close_RoC</th>\n",
              "      <th>Weighted_Price_RoC</th>\n",
              "      <th>RSI</th>\n",
              "      <th>MACD_index</th>\n",
              "      <th>slow_stoch</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:00:00</th>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>31.713233</td>\n",
              "      <td>3678.735005</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:01:00</th>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>31.713233</td>\n",
              "      <td>3678.735005</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:02:00</th>\n",
              "      <td>116.00</td>\n",
              "      <td>116.58</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.58</td>\n",
              "      <td>2.050985</td>\n",
              "      <td>238.357034</td>\n",
              "      <td>116.215883</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004988</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:03:00</th>\n",
              "      <td>116.98</td>\n",
              "      <td>117.00</td>\n",
              "      <td>116.98</td>\n",
              "      <td>117.00</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>2690.890000</td>\n",
              "      <td>116.995217</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.008413</td>\n",
              "      <td>0.003596</td>\n",
              "      <td>0.008413</td>\n",
              "      <td>0.003596</td>\n",
              "      <td>0.006684</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:04:00</th>\n",
              "      <td>117.00</td>\n",
              "      <td>117.00</td>\n",
              "      <td>117.00</td>\n",
              "      <td>117.00</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>5850.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Open    High     Low  ...        RSI  MACD_index  slow_stoch\n",
              "Time                                         ...                                   \n",
              "2013-04-03 00:00:00  116.00  116.00  116.00  ...  33.333333    -0.36038    0.084906\n",
              "2013-04-03 00:01:00  116.00  116.00  116.00  ...  33.333333    -0.36038    0.084906\n",
              "2013-04-03 00:02:00  116.00  116.58  116.00  ...  33.333333    -0.36038    0.084906\n",
              "2013-04-03 00:03:00  116.98  117.00  116.98  ...  33.333333    -0.36038    0.084906\n",
              "2013-04-03 00:04:00  117.00  117.00  117.00  ...  33.333333    -0.36038    0.084906\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjHXkvf-lsLx",
        "colab_type": "text"
      },
      "source": [
        "Dropping Volume Currency as discussed in the Feature Selection phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIlci9pwlsLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DayTrade = DayTrade.drop(columns='Volume_.Currency.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xat-lJpslsL8",
        "colab_type": "code",
        "outputId": "66edcf3e-ee14-4c27-e8a0-d827e8293ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "#We need create a target data, which is basically a copy of the data that will be later shifted\n",
        "target_data = DayTrade.copy()\n",
        "target_data = target_data.iloc[:,5:7]\n",
        "target_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weighted_Price</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:00:00</th>\n",
              "      <td>116.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:01:00</th>\n",
              "      <td>116.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:02:00</th>\n",
              "      <td>116.215883</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:03:00</th>\n",
              "      <td>116.995217</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:04:00</th>\n",
              "      <td>117.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Weighted_Price        date\n",
              "Time                                           \n",
              "2013-04-03 00:00:00      116.000000  2013-04-03\n",
              "2013-04-03 00:01:00      116.000000  2013-04-03\n",
              "2013-04-03 00:02:00      116.215883  2013-04-03\n",
              "2013-04-03 00:03:00      116.995217  2013-04-03\n",
              "2013-04-03 00:04:00      117.000000  2013-04-03"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRCpL0F_lsMF",
        "colab_type": "text"
      },
      "source": [
        "The objective of the model is to predict 10 minutes ahead of the current timestep. The Day Trade data contains the minute to minute data for a total of 1735 days. The analysis must be limited within each day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNwvPFu2lsMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict 10 minutes in the future, although the predictions must be wrapped around each day\n",
        "shift_steps = 10\n",
        "\n",
        "# Now that the target_data was created we need to shift the data so that the target values of 24 hours later aling with our\n",
        "# input data\n",
        "\n",
        "target_data = target_data.groupby('date').shift(-shift_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwr1eLOglsMQ",
        "colab_type": "text"
      },
      "source": [
        "Here we double check that because we shifted the target values now we have NaN values at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkzvDkCtlsMS",
        "colab_type": "code",
        "outputId": "441db2f4-daf0-4727-9a57-22ea1fdf976f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        }
      },
      "source": [
        "target_data.iloc[1420:1450]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weighted_Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:40:00</th>\n",
              "      <td>129.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:41:00</th>\n",
              "      <td>129.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:42:00</th>\n",
              "      <td>129.899861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:43:00</th>\n",
              "      <td>129.892440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:44:00</th>\n",
              "      <td>130.049341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:45:00</th>\n",
              "      <td>131.371316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:46:00</th>\n",
              "      <td>132.534018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:47:00</th>\n",
              "      <td>132.912123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:48:00</th>\n",
              "      <td>132.819273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:49:00</th>\n",
              "      <td>133.091659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:50:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:51:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:52:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:53:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:54:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:55:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:56:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:57:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:58:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:59:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:00:00</th>\n",
              "      <td>164.628270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:01:00</th>\n",
              "      <td>164.971825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:02:00</th>\n",
              "      <td>164.990000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:03:00</th>\n",
              "      <td>164.986254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:04:00</th>\n",
              "      <td>164.998242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:05:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:06:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:07:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:08:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:09:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Weighted_Price\n",
              "Time                               \n",
              "2013-04-03 23:40:00      129.900000\n",
              "2013-04-03 23:41:00      129.900000\n",
              "2013-04-03 23:42:00      129.899861\n",
              "2013-04-03 23:43:00      129.892440\n",
              "2013-04-03 23:44:00      130.049341\n",
              "2013-04-03 23:45:00      131.371316\n",
              "2013-04-03 23:46:00      132.534018\n",
              "2013-04-03 23:47:00      132.912123\n",
              "2013-04-03 23:48:00      132.819273\n",
              "2013-04-03 23:49:00      133.091659\n",
              "2013-04-03 23:50:00             NaN\n",
              "2013-04-03 23:51:00             NaN\n",
              "2013-04-03 23:52:00             NaN\n",
              "2013-04-03 23:53:00             NaN\n",
              "2013-04-03 23:54:00             NaN\n",
              "2013-04-03 23:55:00             NaN\n",
              "2013-04-03 23:56:00             NaN\n",
              "2013-04-03 23:57:00             NaN\n",
              "2013-04-03 23:58:00             NaN\n",
              "2013-04-03 23:59:00             NaN\n",
              "2013-04-08 00:00:00      164.628270\n",
              "2013-04-08 00:01:00      164.971825\n",
              "2013-04-08 00:02:00      164.990000\n",
              "2013-04-08 00:03:00      164.986254\n",
              "2013-04-08 00:04:00      164.998242\n",
              "2013-04-08 00:05:00      164.960000\n",
              "2013-04-08 00:06:00      164.960000\n",
              "2013-04-08 00:07:00      164.960000\n",
              "2013-04-08 00:08:00      164.960000\n",
              "2013-04-08 00:09:00      164.960000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1suUnhXlsMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we need to remove the rows with NaN values for the target data thus needing to exclude also the \n",
        "# 10 lines per day of the DayTrade data\n",
        "\n",
        "DayTrade['target'] = target_data['Weighted_Price']\n",
        "\n",
        "target_data['date'] = DayTrade['date']\n",
        "\n",
        "DayTrade_clean = DayTrade.dropna()\n",
        "target_data_clean = target_data.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OObMTitlsMh",
        "colab_type": "code",
        "outputId": "5a4013c2-8a2c-44be-c689-d32a3f2606e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "DayTrade_clean.shape, target_data_clean.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2481050, 16), (2481050, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "552T9UtMlsMp",
        "colab_type": "text"
      },
      "source": [
        "Number of rows for both data are correct since initially the data consisted of 1735 days of 1440 minutes (total of 2.481.050 rows) and now each day had the last 10 minutes so the total amount of rows must be 1735 days of 1430 minutes (total of 2.481.050)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgUi4ibklsMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the target column was only added to the DayTrade data in order to drop the correct rows.\n",
        "#Removing column 'target' from the DayTrade data\n",
        "\n",
        "DayTrade = DayTrade.drop(columns='target')\n",
        "DayTrade_clean = DayTrade_clean.drop(columns='target')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jmoz-W2lsMy",
        "colab_type": "text"
      },
      "source": [
        "In order to compare results with other models a 90% split is going to be made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT9psvi3lsM0",
        "colab_type": "code",
        "outputId": "8af652da-531e-4672-d514-44e292f59dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "days_in_data = list(dict.fromkeys(DayTrade_clean[\"date\"].values))\n",
        "\n",
        "split = 0.9\n",
        "\n",
        "training_days = days_in_data[:int(split*len(days_in_data))]\n",
        "testing_days = days_in_data[int(split*len(days_in_data)):]\n",
        "\n",
        "print(len(training_days),len(testing_days))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1561 174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIWJr82rlsM7",
        "colab_type": "text"
      },
      "source": [
        "Data will need to be normalized for predictions. A scaler will be fitted for the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZsTpj5PlsOt",
        "colab_type": "code",
        "outputId": "f5efe753-f925-479e-dc95-f7465351485b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#Step 1 - Convert day data into numpy array\n",
        "train_data = np.array(DayTrade_clean.loc[DayTrade_clean.date.isin(training_days),:].drop(columns='date'))\n",
        "label_data = np.array(target_data_clean.loc[target_data_clean.date.isin(training_days),:].drop(columns='date')).reshape(-1,1)\n",
        "\n",
        "#Step 2 - Scale data for Neural Network\n",
        "x_scaler = MinMaxScaler()\n",
        "x_scaler.fit_transform(train_data)\n",
        "y_scaler = MinMaxScaler()\n",
        "y_scaler.fit_transform(label_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00331603],\n",
              "       [0.00331603],\n",
              "       [0.00331603],\n",
              "       ...,\n",
              "       [0.19470373],\n",
              "       [0.19471025],\n",
              "       [0.19481153]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-OEBW-mlsM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_reshape(sequence_length, X_train_scale, Y_train_scale, num_x_signal, num_y_signal):\n",
        "    \"\"\"\n",
        "    Generator function for creating random batches of training-data.\n",
        "    \"\"\"\n",
        "    batch_size = X_train_scale.shape[1] // sequence_length\n",
        "    # Allocate a new array for the batch of input-signals.\n",
        "    x_shape = (batch_size, sequence_length, num_x_signal)\n",
        "    x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
        "    \n",
        " \n",
        "    # Allocate a new array for the batch of output-signals.\n",
        "    y_shape = (batch_size, num_y_signal)\n",
        "    y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
        "\n",
        "    #print(x_batch.shape, y_batch.shape, X_train_scale.shape, Y_train_scale.shape) #debugging\n",
        "    # Create Sequence for sliding window\n",
        "    seq = []\n",
        "    for i in range(batch_size):\n",
        "        seq.append(i*sequence_length)\n",
        "    \n",
        "    # Fill the batch with sequences of data.\n",
        "    for i in range(0,len(seq)-1):\n",
        "\n",
        "        # Copy the sequences of data starting at this index.\n",
        "        x_batch[i] = X_train_scale[0][seq[i]:seq[i]+sequence_length][:]\n",
        "        y_batch[i] = Y_train_scale[0][seq[i]+sequence_length-1][:]\n",
        "        #print(\"iteration: \",i,\"-OK\") #debugging\n",
        "\n",
        "    #print(x_batch.shape,y_batch.shape) #debbuging\n",
        "    return (x_batch, y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gOZQQK9lsNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rand\n",
        "from random import randint\n",
        "from random import seed\n",
        "rand.seed(4)\n",
        "\n",
        "\n",
        "def batch_generator(batch_size, sequence_length, num_x_signal, num_y_signal, train_data, label_data, training_days):\n",
        "    \n",
        "    # Create a Batch function for training data using sliding window technique\n",
        "    # Step 1 - Select a training day\n",
        "\n",
        "    day = rand.choice(training_days)\n",
        "\n",
        "    #Step 2 - Filter Input data and Target with the selected date\n",
        "    day_train_data = train_data[train_data['date'].values==day]\n",
        "    day_label_data = label_data[label_data['date'].values==day]\n",
        "\n",
        "    #Step 3 - Drop date columns from day train data and day label data\n",
        "    day_train_data = day_train_data.drop(columns='date')\n",
        "    day_label_data = day_label_data.drop(columns='date')\n",
        "\n",
        "    #Step 4 - Convert day data into numpy array\n",
        "    day_train_data = np.array(day_train_data)\n",
        "    day_label_data = np.array(day_label_data).reshape(-1,1)\n",
        "\n",
        "    #Step 5 - Scale data for Neural Network\n",
        "    day_train_data = x_scaler.fit_transform(day_train_data)\n",
        "    day_label_data = y_scaler.fit_transform(day_label_data)\n",
        "\n",
        "    #Step 6 - Reshape data to fit keras requirement to have a (x,y,z) shape\n",
        "    day_train_data = day_train_data.reshape(1,day_train_data.shape[0],day_train_data.shape[1])\n",
        "    day_label_data = day_label_data.reshape(1,day_label_data.shape[0],day_label_data.shape[1])\n",
        "    \n",
        "     #Step 7 - Reshape data into Batches\n",
        "    day_train_data_reshape , day_label_data_reshape =  batch_reshape(sequence_length, day_train_data, day_label_data, num_x_signal, num_y_signal)\n",
        "    \n",
        "    # print(day_train_data_reshape.shape , day_train_data_reshape.shape)#debugging\n",
        "   \n",
        "    #Step 8 - Apply the \"jumping\" slidding window technique to the reshaped date\n",
        "    # Infinite loop.\n",
        "    while True:\n",
        "        # Allocate a new array for the batch of input-signals.\n",
        "        x_shape = (batch_size, sequence_length, num_x_signal)\n",
        "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
        "\n",
        "        # Allocate a new array for the batch of output-signals.\n",
        "        y_shape = (batch_size, num_y_signal)\n",
        "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
        "\n",
        "        # Fill the batch with random continuous sequences of data.\n",
        "\n",
        "        # Get a random start-index.\n",
        "        # This points somewhere into the training-data.\n",
        "        idx = np.random.randint(day_train_data_reshape.shape[0] - batch_size)\n",
        "\n",
        "        # Copy the sequences of data starting at this index.\n",
        "        x_batch = day_train_data_reshape[idx:idx+batch_size]\n",
        "        y_batch = day_label_data_reshape[idx:idx+batch_size]\n",
        "\n",
        "\n",
        "        yield (x_batch, y_batch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjsNJ6OVlsNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_x_signal = 14 # number of input features\n",
        "num_y_signal = 1 # number of label classes\n",
        "\n",
        "batch_size = 50 # tunning parameter\n",
        "sequence_length = 25 #Amount of time-steps to look back for the 10 minute prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyewDvW6lsNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = batch_generator(batch_size,sequence_length, num_x_signal, num_y_signal, DayTrade_clean, target_data_clean,training_days)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCbJykW9lsNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_batch, y_batch = next(generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRGV8k-ElsNi",
        "colab_type": "code",
        "outputId": "f6c9b8ff-d3f6-4977-b382-88cb46f44e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(x_batch.shape)\n",
        "print(y_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 25, 14)\n",
            "(50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a97sQHTwlsNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def batch_validation(sequence_length, num_x_signal, num_y_signal, test_data, label_test_data, testing_days):\n",
        "    \n",
        "    # Create a Batch function for validation data using sliding window technique\n",
        "    # Step 1 - Select a testing day\n",
        "\n",
        "    day = rand.choice(testing_days)\n",
        "\n",
        "    #Step 2 - Filter Input data and Target with the selected date\n",
        "    day_test_data = test_data[test_data['date'].values==day]\n",
        "    day_label_data = label_test_data[label_test_data['date'].values==day]\n",
        "\n",
        "    #Step 3 - Drop date columns from day test data and day label data\n",
        "    day_test_data = day_test_data.drop(columns='date')\n",
        "    day_label_data = day_label_data.drop(columns='date')\n",
        "\n",
        "    #Step 4 - Convert day data into numpy array\n",
        "    day_test_data = np.array(day_test_data)\n",
        "    day_label_data = np.array(day_label_data).reshape(-1,1)\n",
        "\n",
        "    #Step 5 - Scale data for Neural Network\n",
        "    day_test_data = x_scaler.fit_transform(day_test_data)\n",
        "    day_label_data = y_scaler.fit_transform(day_label_data)\n",
        "\n",
        "    #Step 6 - Reshape data to fit keras requirement to have a (x,y,z) shape\n",
        "    day_test_data = day_test_data.reshape(1,day_test_data.shape[0],day_test_data.shape[1])\n",
        "    day_label_data = day_label_data.reshape(1,day_label_data.shape[0],day_label_data.shape[1])\n",
        "    \n",
        "    #print(day_test_data.shape , day_label_data.shape)#debugging\n",
        "    \n",
        "    #Step 7 - Reshape data into Batches using slidding window   \n",
        "    batch_val_size = day_test_data.shape[1] - sequence_length\n",
        "    \n",
        "    x_val_shape = (batch_val_size, sequence_length, num_x_signal)\n",
        "    x_batch = np.zeros(shape=x_val_shape, dtype=np.float16)\n",
        "        \n",
        "    y_val_shape = (batch_val_size, num_y_signal)\n",
        "    y_batch = np.zeros(shape=y_val_shape, dtype=np.float16)\n",
        "    \n",
        "    #print(x_batch.shape, y_batch.shape) # debugging\n",
        "    for i in range(batch_val_size):\n",
        "\n",
        "        # Copy the sequences of data starting at this index.\n",
        "        x_batch[i] = day_test_data[0][i:i+sequence_length][:]\n",
        "        y_batch[i] = day_label_data[0][i+sequence_length-1][:]\n",
        "\n",
        "    \n",
        "    return (x_batch, y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHB0sxjvlsNu",
        "colab_type": "code",
        "outputId": "47c6f3b6-f775-4cb6-c5dc-88c743753ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_val, Y_val = batch_validation(sequence_length,num_x_signal, num_y_signal, DayTrade_clean, target_data_clean, testing_days)\n",
        "print(X_val.shape, Y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1405, 25, 14) (1405, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JijIrUClsN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = (X_val, Y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxrLHSOZlsN8",
        "colab_type": "text"
      },
      "source": [
        "## Create Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccoKnJPolsN-",
        "colab_type": "code",
        "outputId": "26c6a2fc-f088-45c8-cc5f-c624b722d8db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from keras.constraints import max_norm\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=200,\n",
        "              return_sequences=True,\n",
        "              input_shape=(None,num_x_signal,)))\n",
        "model.add(LSTM(units=150, return_sequences=False))\n",
        "model.add(Dense(num_y_signal,activation='relu'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, None, 200)         172000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 150)               210600    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 151       \n",
            "=================================================================\n",
            "Total params: 382,751\n",
            "Trainable params: 382,751\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3GwP4Z4lsOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(lr=1e-3)\n",
        "\n",
        "model.compile(loss=losses.mean_squared_error, optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U37PF2alsOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "model_file = \"LSTM 2 layers_Relu_MinMax_modified_600epochs.h5\"\n",
        "\n",
        "mc = ModelCheckpoint(model_file, monitor=\"val_loss\", mode=\"min\", save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=4, min_lr=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Ohrxs6lsOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "history = model.fit_generator(generator=generator,\n",
        "                    epochs=2000,\n",
        "                    steps_per_epoch=50,\n",
        "                    validation_data=validation_data,\n",
        "                    callbacks=[ mc, reduce_lr])\n",
        "                    #callbacks=[es, reduce_lr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDwrXnRqlsOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_df = pd.DataFrame(history.history) \n",
        "hist_csv_file = model_file.split('.')[0]+'.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe87e17SmdU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saveDrivePath = '/content/drive/My Drive/Capstone/Approach/Step2DayTrade_ver3'\n",
        "h5FilePath = '/content/'+ model_file\n",
        "historyFilePath = '/content/' + hist_csv_file\n",
        "\n",
        "os.system(\"mv \"+'\"'+h5FilePath+'\"' + \" \" + '\"'+saveDrivePath+'\"' )\n",
        "os.system(\"mv \"+'\"'+historyFilePath+'\"' + \" \" + '\"'+saveDrivePath+'\"' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxUwF33QY49I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "historyFilePath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_OeIoQnlsOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjPIJsCGmOAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzcemWUVlsOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The algorithm uses data of the previous 25 time-steps to forecast the following 10th minute into the future. Therefore,  \n",
        "# the first prediction of every day can only be excecuted after 25 minutes of the begining of the day, and will refer  \n",
        "# to the prediction of the minute 35.\n",
        "\n",
        "# Since every day consists of 1440 minutes the algorithm is only able to predict the last 1405 minutes of the day.\n",
        "\n",
        "#Create a Dataframe to hold the true predicted values for each day.\n",
        "predictionsData = DayTrade.copy()\n",
        "predictionsData = predictionsData.loc[:,['Weighted_Price','date']]\n",
        "predictionsData = predictionsData[predictionsData['date'].isin(testing_days)]# Filter only the testing days\n",
        "predictionsData = predictionsData.groupby('date').apply(lambda group: group.iloc[35:])# For each day filter the last 1405 minutes (1440 - 1405 = 35)\n",
        "predictionsData.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNIHNgrDlsOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(modelFilename):\n",
        "\n",
        "    column_name = modelFilename.split('.')[0]\n",
        "    #Step 1 - Create a column to hold predictions\n",
        "    predictionsData[column_name] = np.nan \n",
        "    \n",
        "    #Step 2 - load model for prediction   \n",
        "    loaded_model = tf.keras.models.load_model(modelFilename)\n",
        "    \n",
        "    for day in testing_days:\n",
        "        #Step 3 - Filter Input data and Target with the selected date\n",
        "        day_test_data = DayTrade_clean[DayTrade_clean['date'].values==day]\n",
        "\n",
        "        #Step 4 - Drop date column\n",
        "        day_test_data = day_test_data.drop(columns='date')\n",
        "\n",
        "        #Step 5 - Convert day data into numpy array\n",
        "        day_test_data = np.array(day_test_data)\n",
        "\n",
        "        #Step 6 - Scale data for Neural Network\n",
        "        day_test_data = x_scaler.fit_transform(day_test_data)\n",
        "\n",
        "        #Step 7 - Reshape data to fit keras requirement to have a (x,y,z) shape\n",
        "        day_test_data = day_test_data.reshape(1,day_test_data.shape[0],day_test_data.shape[1])\n",
        "\n",
        "        #Step 8 - Reshape data into Batches using slidding window   \n",
        "        batch_val_size = day_test_data.shape[1] - sequence_length\n",
        "\n",
        "        x_val_shape = (batch_val_size, sequence_length, num_x_signal)\n",
        "        x_batch = np.zeros(shape=x_val_shape, dtype=np.float16)\n",
        "\n",
        "        for i in range(batch_val_size):\n",
        "\n",
        "            # Copy the sequences of data starting at this index.\n",
        "            x_batch[i] = day_test_data[0][i:i+sequence_length][:]\n",
        "\n",
        "        #Step 9 - Generate the prediction for that day   \n",
        "        ypred = loaded_model.predict(x_batch)\n",
        "        ypred_rescaled = y_scaler.inverse_transform(ypred)\n",
        "\n",
        "        #Step 10 - Copy the prediction values to the correspondent day in the predictionData\n",
        "        predictionsData.loc[predictionsData['date']==day,column_name] = ypred_rescaled\n",
        "    \n",
        "    return(predictionsData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fU5aP2blsO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelPath = '/content/drive/My Drive/Capstone/Approach/Step2DayTrade_ver3'\n",
        "modelfilePath = modelPath + '/' + model_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoCmeTw8lsPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(modelfilePath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s49G2-L4wTOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Dataframe with prediction as csv file\n",
        "prediction_file = 'predictions_' +  model_file.split('.')[0] + '.csv'\n",
        "prediction_filePath = '/content/' + prediction_file\n",
        "predictionsData.to_csv(prediction_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgoSnGyswuJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "destinantionDir = '/content/drive/My Drive/Capstone/Approach/Step2DayTrade_ver3/Predictions_csv'\n",
        "oscmd = \"mv \"+'\"'+prediction_filePath+'\"' + \" \" + '\"'+destinantionDir+'\"' \n",
        "oscmd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSHWElxsIWJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move csv files to directory in Drive\n",
        "os.system(oscmd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud9T-Pf5Ia0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}