{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "DayTradeSingle_DFSDInput.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmaciver/Ryerson_Capstone/blob/master/Approach/Step4-DayTade_DFSDInputs/DayTradeSingle_DFSDInput.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9hHMfSFdQAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sFpaBQvcURQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyHGVUHWcUSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, LSTM, TimeDistributed, Lambda, Dropout\n",
        "from tensorflow.python.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras import losses\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random as rand\n",
        "from random import randint\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3LxzHxTcUSQ",
        "colab_type": "code",
        "outputId": "d0b13777-9d0b-4814-eccd-9740d805a728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "file_path = \"/content/drive/My Drive/Capstone/Data Exploration/Day_trade_data.csv\"\n",
        "DayTrade = pd.read_csv(file_path, index_col='Time')\n",
        "DayTrade = DayTrade.drop([DayTrade.columns[0]] ,  axis='columns')\n",
        "DayTrade.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_.BTC.</th>\n",
              "      <th>Volume_.Currency.</th>\n",
              "      <th>Weighted_Price</th>\n",
              "      <th>date</th>\n",
              "      <th>Open_RoC</th>\n",
              "      <th>High_RoC</th>\n",
              "      <th>Low_RoC</th>\n",
              "      <th>Close_RoC</th>\n",
              "      <th>Weighted_Price_RoC</th>\n",
              "      <th>RSI</th>\n",
              "      <th>MACD_index</th>\n",
              "      <th>slow_stoch</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:00:00</th>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>31.713233</td>\n",
              "      <td>3678.735005</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:01:00</th>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.00</td>\n",
              "      <td>31.713233</td>\n",
              "      <td>3678.735005</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:02:00</th>\n",
              "      <td>116.00</td>\n",
              "      <td>116.58</td>\n",
              "      <td>116.00</td>\n",
              "      <td>116.58</td>\n",
              "      <td>2.050985</td>\n",
              "      <td>238.357034</td>\n",
              "      <td>116.215883</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004988</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:03:00</th>\n",
              "      <td>116.98</td>\n",
              "      <td>117.00</td>\n",
              "      <td>116.98</td>\n",
              "      <td>117.00</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>2690.890000</td>\n",
              "      <td>116.995217</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.008413</td>\n",
              "      <td>0.003596</td>\n",
              "      <td>0.008413</td>\n",
              "      <td>0.003596</td>\n",
              "      <td>0.006684</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:04:00</th>\n",
              "      <td>117.00</td>\n",
              "      <td>117.00</td>\n",
              "      <td>117.00</td>\n",
              "      <td>117.00</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>5850.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>-0.36038</td>\n",
              "      <td>0.084906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Open    High     Low  ...        RSI  MACD_index  slow_stoch\n",
              "Time                                         ...                                   \n",
              "2013-04-03 00:00:00  116.00  116.00  116.00  ...  33.333333    -0.36038    0.084906\n",
              "2013-04-03 00:01:00  116.00  116.00  116.00  ...  33.333333    -0.36038    0.084906\n",
              "2013-04-03 00:02:00  116.00  116.58  116.00  ...  33.333333    -0.36038    0.084906\n",
              "2013-04-03 00:03:00  116.98  117.00  116.98  ...  33.333333    -0.36038    0.084906\n",
              "2013-04-03 00:04:00  117.00  117.00  117.00  ...  33.333333    -0.36038    0.084906\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxN7_raMcUSi",
        "colab_type": "text"
      },
      "source": [
        "Dropping Volume Currency as discussed in the Feature Selection phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDc3AcnhcUSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DayTrade = DayTrade.drop(columns='Volume_.Currency.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYfTiUg7cUS0",
        "colab_type": "code",
        "outputId": "8cb05f77-e405-44a3-a4fe-a985b262f61f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "#We need create a target data, which is basically a copy of the data that will be later shifted\n",
        "target_data = DayTrade.copy()\n",
        "target_data = target_data.iloc[:,5:7]\n",
        "target_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weighted_Price</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:00:00</th>\n",
              "      <td>116.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:01:00</th>\n",
              "      <td>116.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:02:00</th>\n",
              "      <td>116.215883</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:03:00</th>\n",
              "      <td>116.995217</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 00:04:00</th>\n",
              "      <td>117.000000</td>\n",
              "      <td>2013-04-03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Weighted_Price        date\n",
              "Time                                           \n",
              "2013-04-03 00:00:00      116.000000  2013-04-03\n",
              "2013-04-03 00:01:00      116.000000  2013-04-03\n",
              "2013-04-03 00:02:00      116.215883  2013-04-03\n",
              "2013-04-03 00:03:00      116.995217  2013-04-03\n",
              "2013-04-03 00:04:00      117.000000  2013-04-03"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gqzrRLVcUTB",
        "colab_type": "text"
      },
      "source": [
        "The objective of the model is to predict 10 minutes ahead of the current timestep. The Day Trade data contains the minute to minute data for a total of 1735 days. The analysis must be limited within each day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlV_1_cdcUTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict 10 minutes in the future, although the predictions must be wrapped around each day\n",
        "shift_steps = 10\n",
        "\n",
        "# Now that the target_data was created we need to shift the data so that the target values of 24 hours later aling with our\n",
        "# input data\n",
        "\n",
        "target_data = target_data.groupby('date').shift(-shift_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PabWRzUIcUTV",
        "colab_type": "text"
      },
      "source": [
        "Here we double check that because we shifted the target values now we have NaN values at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP86VkS7cUTa",
        "colab_type": "code",
        "outputId": "6216fe91-c6bf-41cb-c21f-2bc3c4aa6576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        }
      },
      "source": [
        "target_data.iloc[1420:1450]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weighted_Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:40:00</th>\n",
              "      <td>129.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:41:00</th>\n",
              "      <td>129.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:42:00</th>\n",
              "      <td>129.899861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:43:00</th>\n",
              "      <td>129.892440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:44:00</th>\n",
              "      <td>130.049341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:45:00</th>\n",
              "      <td>131.371316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:46:00</th>\n",
              "      <td>132.534018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:47:00</th>\n",
              "      <td>132.912123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:48:00</th>\n",
              "      <td>132.819273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:49:00</th>\n",
              "      <td>133.091659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:50:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:51:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:52:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:53:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:54:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:55:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:56:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:57:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:58:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-03 23:59:00</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:00:00</th>\n",
              "      <td>164.628270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:01:00</th>\n",
              "      <td>164.971825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:02:00</th>\n",
              "      <td>164.990000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:03:00</th>\n",
              "      <td>164.986254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:04:00</th>\n",
              "      <td>164.998242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:05:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:06:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:07:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:08:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-08 00:09:00</th>\n",
              "      <td>164.960000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Weighted_Price\n",
              "Time                               \n",
              "2013-04-03 23:40:00      129.900000\n",
              "2013-04-03 23:41:00      129.900000\n",
              "2013-04-03 23:42:00      129.899861\n",
              "2013-04-03 23:43:00      129.892440\n",
              "2013-04-03 23:44:00      130.049341\n",
              "2013-04-03 23:45:00      131.371316\n",
              "2013-04-03 23:46:00      132.534018\n",
              "2013-04-03 23:47:00      132.912123\n",
              "2013-04-03 23:48:00      132.819273\n",
              "2013-04-03 23:49:00      133.091659\n",
              "2013-04-03 23:50:00             NaN\n",
              "2013-04-03 23:51:00             NaN\n",
              "2013-04-03 23:52:00             NaN\n",
              "2013-04-03 23:53:00             NaN\n",
              "2013-04-03 23:54:00             NaN\n",
              "2013-04-03 23:55:00             NaN\n",
              "2013-04-03 23:56:00             NaN\n",
              "2013-04-03 23:57:00             NaN\n",
              "2013-04-03 23:58:00             NaN\n",
              "2013-04-03 23:59:00             NaN\n",
              "2013-04-08 00:00:00      164.628270\n",
              "2013-04-08 00:01:00      164.971825\n",
              "2013-04-08 00:02:00      164.990000\n",
              "2013-04-08 00:03:00      164.986254\n",
              "2013-04-08 00:04:00      164.998242\n",
              "2013-04-08 00:05:00      164.960000\n",
              "2013-04-08 00:06:00      164.960000\n",
              "2013-04-08 00:07:00      164.960000\n",
              "2013-04-08 00:08:00      164.960000\n",
              "2013-04-08 00:09:00      164.960000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zBH2un3cUTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we need to remove the rows with NaN values for the target data thus needing to exclude also the \n",
        "# 10 lines per day of the DayTrade data\n",
        "\n",
        "DayTrade['target'] = target_data['Weighted_Price']\n",
        "\n",
        "target_data['date'] = DayTrade['date']\n",
        "\n",
        "DayTrade_clean = DayTrade.dropna()\n",
        "target_data_clean = target_data.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "876etTVLcUTy",
        "colab_type": "code",
        "outputId": "e8d57934-a827-4feb-84fa-e5d4548b2f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "DayTrade_clean.shape, target_data_clean.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2481050, 16), (2481050, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaFi_FnRcUUK",
        "colab_type": "text"
      },
      "source": [
        "Number of rows for both data are correct since initially the data consisted of 1735 days of 1440 minutes (total of 2.481.050 rows) and now each day had the last 10 minutes so the total amount of rows must be 1735 days of 1430 minutes (total of 2.481.050)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_8nT8cycUUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the target column was only added to the DayTrade data in order to drop the correct rows.\n",
        "#Removing column 'target' from the DayTrade data\n",
        "\n",
        "DayTrade = DayTrade.drop(columns='target')\n",
        "DayTrade_clean = DayTrade_clean.drop(columns='target')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq_qUZNwcUUq",
        "colab_type": "text"
      },
      "source": [
        "## Adding prediction for High and Low based on Daily Summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsP1JPiQcUU0",
        "colab_type": "text"
      },
      "source": [
        "The daily summary was used to predict the high and low price of the next day. This values will then be used by the algorithm to try to reduce the variance of the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7TgM2dmcUU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each day of the DayTrade data must have a prediction for what is the current day high and low price expectation\n",
        "\n",
        "#Load DailySummary data\n",
        "file_path = \"/content/drive/My Drive/Capstone/Data Exploration/DSFD.csv\"\n",
        "DaySummary = pd.read_csv(file_path, index_col='date')\n",
        "DaySummary = DaySummary.drop([DaySummary.columns[0]] ,  axis='columns')\n",
        "DaySummary = DaySummary.drop(columns=['Volume_Currency','Close_RoC'])\n",
        "\n",
        "target_dataDaySummary = DaySummary.copy()\n",
        "target_dataDaySummary = target_dataDaySummary.loc[:,['High','Low']]\n",
        "\n",
        "shift_steps = 1\n",
        "target_dataDaySummary = target_dataDaySummary.shift(-shift_steps)\n",
        "\n",
        "DaySummary_clean = DaySummary.iloc[:-1,:]\n",
        "target_dataDaySummary_clean = target_dataDaySummary.iloc[:-1,:]\n",
        "\n",
        "X_data = np.array(DaySummary_clean)\n",
        "Y_data = np.array(target_dataDaySummary_clean)\n",
        "\n",
        "train_split = 0.9\n",
        "\n",
        "n_train_rows = int(X_data.shape[0]*train_split)\n",
        "\n",
        "X_train = X_data[0:n_train_rows]\n",
        "Y_train = Y_data[0:n_train_rows]\n",
        "\n",
        "#Create Scalers and fit them\n",
        "x_scaler = MinMaxScaler()\n",
        "x_scaler.fit(X_train)\n",
        "\n",
        "y_scaler = MinMaxScaler()\n",
        "y_scaler.fit(Y_train)\n",
        "\n",
        "\n",
        "# Scale X_data, rechape and transform it into Batches for prediction\n",
        "X_data_scaled = x_scaler.transform(X_data)\n",
        "X_data_scaled  = X_data_scaled.reshape(1,X_data_scaled.shape[0],X_data_scaled.shape[1])\n",
        "\n",
        "#Create Batches for the X_data\n",
        "sequence_length = 25\n",
        "num_x_signal = 13\n",
        "num_y_signal = 2\n",
        "\n",
        "# Allocate a new array for the batch of input-signals.\n",
        "batch_size_val = X_data_scaled.shape[1] - sequence_length \n",
        "\n",
        "x_shape = (batch_size_val, sequence_length, num_x_signal)\n",
        "x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
        "\n",
        "# Fill the batch with random sequences of data.\n",
        "for i in range(batch_size_val):\n",
        "    # Copy the sequences of data starting at this index.\n",
        "    x_batch[i] = X_data_scaled[0][i:i+sequence_length][:]\n",
        "    \n",
        "#load model for prediction \n",
        "DSFD_modelFilePath = \"/content/drive/My Drive/Capstone/Approach/Step4DayTrade_DSFDInput/DSFD_H5/DailySummary_LSTM 2 layers_Relu_MinMax_Modified_LogCosh.h5\"\n",
        "loaded_model = tf.keras.models.load_model(DSFD_modelFilePath)  \n",
        "\n",
        "#Generate the prediction   \n",
        "ypred = loaded_model.predict(x_batch)\n",
        "ypred_rescaled = y_scaler.inverse_transform(ypred)\n",
        "\n",
        "#Create columns of prediction for DaySummary data\n",
        "DaySummary['High_Pred'] = np.nan\n",
        "DaySummary['Low_Pred'] = np.nan\n",
        "\n",
        "\n",
        "#Copy the prediction values to the DauSummary Data\n",
        "DaySummary.High_Pred[26:] =  ypred_rescaled[:,0]\n",
        "DaySummary.Low_Pred[26:] =  ypred_rescaled[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJDrtTggcUVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now that DaySummary has predictions High and Low predictions for the day, it is necessary to copy those prediction into the \n",
        "# DayTrade data\n",
        "\n",
        "DaySummary_clean = DaySummary.copy()\n",
        "DaySummary_clean.dropna()\n",
        "\n",
        "days_in_DailySummary = list(dict.fromkeys(DaySummary_clean.index.values))\n",
        "days_in_data = list(dict.fromkeys(DayTrade_clean[\"date\"].values))\n",
        "days_with_predictions = list(set(days_in_data).intersection(days_in_DailySummary))\n",
        "\n",
        "DayTrade_clean['High_Pred'] = np.nan\n",
        "DayTrade_clean['Low_Pred'] = np.nan\n",
        "\n",
        "for day in days_with_predictions:\n",
        "    DayTrade_clean.High_Pred[DayTrade_clean['date'].values==day] = np.array(DaySummary_clean.High_Pred[DaySummary_clean.index==day])\n",
        "    DayTrade_clean.Low_Pred[DayTrade_clean['date'].values==day] = np.array(DaySummary_clean.Low_Pred[DaySummary_clean.index==day])    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqtNi3UPcUVO",
        "colab_type": "code",
        "outputId": "9930f2ae-0aab-4c3d-a1b7-b7b43ea454f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "DayTrade_clean = DayTrade_clean.dropna()\n",
        "DayTrade_clean.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2453880, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QARSmHyKcUVc",
        "colab_type": "text"
      },
      "source": [
        "By adding the High and Low predictions from the Daily Summary data and removing the NaN the data now has 2.453.880 rows (which corresponds to 1716 days), which means 19 days of the original data lied before the 25 first days that the Daily Summary data needed to begin making predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHAbwIhTcUVf",
        "colab_type": "text"
      },
      "source": [
        "## Continuing with Day Trade Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fz9_o3BcUVh",
        "colab_type": "text"
      },
      "source": [
        "In order to compare results with other models the same dates for testing must be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Dc-RCkcUVk",
        "colab_type": "code",
        "outputId": "b60a4ff3-6f6b-4b57-edaf-95fcdd3bca02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Previous algorithm used 174 days for testing\n",
        "\n",
        "training_days = days_in_data[:len(days_in_data)-174]\n",
        "testing_days = days_in_data[len(days_in_data)-174:]\n",
        "\n",
        "print(len(training_days),len(testing_days))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1561 174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOgX6izscUVs",
        "colab_type": "text"
      },
      "source": [
        "Data will need to be normalized for predictions. A scaler will be fitted for the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIPe-9dwcUVu",
        "colab_type": "code",
        "outputId": "43fc2167-0cc1-426d-c3ec-92620dc2d6d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Step 1 - Convert day data into numpy array\n",
        "train_data = np.array(DayTrade_clean.loc[DayTrade_clean.date.isin(training_days),:].drop(columns='date'))\n",
        "label_data = np.array(target_data_clean.loc[target_data_clean.date.isin(training_days),:].drop(columns='date')).reshape(-1,1)\n",
        "\n",
        "#Step 2 - Scale data for Neural Network\n",
        "x_scaler = MinMaxScaler()\n",
        "x_scaler.fit(train_data)\n",
        "y_scaler = MinMaxScaler()\n",
        "y_scaler.fit(label_data)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp11hzjlcUV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_reshape(sequence_length, X_train_scale, Y_train_scale, num_x_signal, num_y_signal):\n",
        "    \"\"\"\n",
        "    Generator function for creating random batches of training-data.\n",
        "    \"\"\"\n",
        "    batch_size = X_train_scale.shape[1] // sequence_length\n",
        "    # Allocate a new array for the batch of input-signals.\n",
        "    x_shape = (batch_size, sequence_length, num_x_signal)\n",
        "    x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
        "    \n",
        " \n",
        "    # Allocate a new array for the batch of output-signals.\n",
        "    y_shape = (batch_size, num_y_signal)\n",
        "    y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
        "\n",
        "    #print(x_batch.shape, y_batch.shape, X_train_scale.shape, Y_train_scale.shape) #debugging\n",
        "    # Create Sequence for sliding window\n",
        "    seq = []\n",
        "    for i in range(batch_size):\n",
        "        seq.append(i*sequence_length)\n",
        "    \n",
        "    # Fill the batch with sequences of data.\n",
        "    for i in range(0,len(seq)-1):\n",
        "\n",
        "        # Copy the sequences of data starting at this index.\n",
        "        x_batch[i] = X_train_scale[0][seq[i]:seq[i]+sequence_length][:]\n",
        "        y_batch[i] = Y_train_scale[0][seq[i]+sequence_length-1][:]\n",
        "        #print(\"iteration: \",i,\"-OK\") #debugging\n",
        "\n",
        "    #print(x_batch.shape,y_batch.shape) #debbuging\n",
        "    return (x_batch, y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqAUptx1cUV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_generator(batch_size, sequence_length, num_x_signal, num_y_signal, train_data, label_data, training_days):\n",
        "    \n",
        "    # Create a Batch function for training data using sliding window technique\n",
        "    # Step 1 - Select a training day\n",
        "\n",
        "    day = rand.choice(training_days)\n",
        "\n",
        "    #Step 2 - Filter Input data and Target with the selected date\n",
        "    day_train_data = train_data[train_data['date'].values==day]\n",
        "    day_label_data = label_data[label_data['date'].values==day]\n",
        "\n",
        "    #Step 3 - Drop date columns from day train data and day label data\n",
        "    day_train_data = day_train_data.drop(columns='date')\n",
        "    day_label_data = day_label_data.drop(columns='date')\n",
        "\n",
        "    #Step 4 - Convert day data into numpy array\n",
        "    day_train_data = np.array(day_train_data)\n",
        "    day_label_data = np.array(day_label_data).reshape(-1,1)\n",
        "\n",
        "    #Step 5 - Scale data for Neural Network\n",
        "    day_train_data = x_scaler.transform(day_train_data)\n",
        "    day_label_data = y_scaler.transform(day_label_data)\n",
        "\n",
        "    #Step 6 - Reshape data to fit keras requirement to have a (x,y,z) shape\n",
        "    day_train_data = day_train_data.reshape(1,day_train_data.shape[0],day_train_data.shape[1])\n",
        "    day_label_data = day_label_data.reshape(1,day_label_data.shape[0],day_label_data.shape[1])\n",
        "    \n",
        "     #Step 7 - Reshape data into Batches\n",
        "    day_train_data_reshape , day_label_data_reshape =  batch_reshape(sequence_length, day_train_data, day_label_data, num_x_signal, num_y_signal)\n",
        "    \n",
        "    # print(day_train_data_reshape.shape , day_train_data_reshape.shape)#debugging\n",
        "   \n",
        "    #Step 8 - Apply the \"jumping\" slidding window technique to the reshaped date\n",
        "    # Infinite loop.\n",
        "    while True:\n",
        "        # Allocate a new array for the batch of input-signals.\n",
        "        x_shape = (batch_size, sequence_length, num_x_signal)\n",
        "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
        "\n",
        "        # Allocate a new array for the batch of output-signals.\n",
        "        y_shape = (batch_size, num_y_signal)\n",
        "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
        "\n",
        "        # Fill the batch with random continuous sequences of data.\n",
        "\n",
        "        # Get a random start-index.\n",
        "        # This points somewhere into the training-data.\n",
        "        idx = np.random.randint(day_train_data_reshape.shape[0] - batch_size)\n",
        "\n",
        "        # Copy the sequences of data starting at this index.\n",
        "        x_batch = day_train_data_reshape[idx:idx+batch_size]\n",
        "        y_batch = day_label_data_reshape[idx:idx+batch_size]\n",
        "\n",
        "\n",
        "        yield (x_batch, y_batch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrBCLFiocUWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_x_signal = 16 # number of input features\n",
        "num_y_signal = 1 # number of label classes\n",
        "\n",
        "batch_size = 50 # tunning parameter\n",
        "sequence_length = 25 #Amount of time-steps to look back for the 10 minute prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcUBg8DJcUWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = batch_generator(batch_size,sequence_length, num_x_signal, num_y_signal, DayTrade_clean, target_data_clean,training_days)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e20qDTDZcUWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_batch, y_batch = next(generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNAD0wbrcUW4",
        "colab_type": "code",
        "outputId": "f82ca86c-e71c-4271-9fe5-18257350aa13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(x_batch.shape)\n",
        "print(y_batch.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 25, 16)\n",
            "(50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HcId0fVcUXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def batch_validation(sequence_length, num_x_signal, num_y_signal, test_data, label_test_data, testing_days):\n",
        "    \n",
        "    # Create a Batch function for validation data using sliding window technique\n",
        "    # Step 1 - Select a testing day\n",
        "\n",
        "    day = rand.choice(testing_days)\n",
        "\n",
        "    #Step 2 - Filter Input data and Target with the selected date\n",
        "    day_test_data = test_data[test_data['date'].values==day]\n",
        "    day_label_data = label_test_data[label_test_data['date'].values==day]\n",
        "\n",
        "    #Step 3 - Drop date columns from day test data and day label data\n",
        "    day_test_data = day_test_data.drop(columns='date')\n",
        "    day_label_data = day_label_data.drop(columns='date')\n",
        "\n",
        "    #Step 4 - Convert day data into numpy array\n",
        "    day_test_data = np.array(day_test_data)\n",
        "    day_label_data = np.array(day_label_data).reshape(-1,1)\n",
        "\n",
        "    #Step 5 - Scale data for Neural Network\n",
        "    day_test_data = x_scaler.transform(day_test_data)\n",
        "    day_label_data = y_scaler.transform(day_label_data)\n",
        "\n",
        "    #Step 6 - Reshape data to fit keras requirement to have a (x,y,z) shape\n",
        "    day_test_data = day_test_data.reshape(1,day_test_data.shape[0],day_test_data.shape[1])\n",
        "    day_label_data = day_label_data.reshape(1,day_label_data.shape[0],day_label_data.shape[1])\n",
        "    \n",
        "    #print(day_test_data.shape , day_label_data.shape)#debugging\n",
        "    \n",
        "    #Step 7 - Reshape data into Batches using slidding window   \n",
        "    batch_val_size = day_test_data.shape[1] - sequence_length\n",
        "    \n",
        "    x_val_shape = (batch_val_size, sequence_length, num_x_signal)\n",
        "    x_batch = np.zeros(shape=x_val_shape, dtype=np.float16)\n",
        "        \n",
        "    y_val_shape = (batch_val_size, num_y_signal)\n",
        "    y_batch = np.zeros(shape=y_val_shape, dtype=np.float16)\n",
        "    \n",
        "    #print(x_batch.shape, y_batch.shape) # debugging\n",
        "    for i in range(batch_val_size):\n",
        "\n",
        "        # Copy the sequences of data starting at this index.\n",
        "        x_batch[i] = day_test_data[0][i:i+sequence_length][:]\n",
        "        y_batch[i] = day_label_data[0][i+sequence_length-1][:]\n",
        "\n",
        "    \n",
        "    return (x_batch, y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm574sBxcUXN",
        "colab_type": "code",
        "outputId": "57f534f4-e314-48cc-8e06-abc3cefe4f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_val, Y_val = batch_validation(sequence_length,num_x_signal, num_y_signal, DayTrade_clean, target_data_clean, testing_days)\n",
        "print(X_val.shape, Y_val.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1405, 25, 16) (1405, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWbRxaZycUXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = (X_val, Y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT3Rc3ffcUXs",
        "colab_type": "text"
      },
      "source": [
        "## Create Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SbFQa1xcUXy",
        "colab_type": "code",
        "outputId": "5cf56aad-0c34-4196-bd2a-1a889b2d0081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=200,\n",
        "              return_sequences=True,\n",
        "              input_shape=(None,num_x_signal,)))\n",
        "model.add(LSTM(units=150, return_sequences=False))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_y_signal,activation='linear'))\n",
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, None, 200)         173600    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 150)               210600    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 150)               600       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 151       \n",
            "=================================================================\n",
            "Total params: 384,951\n",
            "Trainable params: 384,651\n",
            "Non-trainable params: 300\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4I_NPSscUYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(lr=1e-3)\n",
        "\n",
        "model.compile(loss=losses.logcosh, optimizer=optimizer)\n",
        "\n",
        "#model.save_weights('initial_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCizhpRZcUYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_file = \"predictions_Step4_GRU 2 layers_Relu_MinMax_logcosh.h5\"\n",
        "\n",
        "mc = ModelCheckpoint(model_file, monitor=\"val_loss\", mode=\"min\", save_best_only=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=4, min_lr=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csq6lk56cUYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "model.load_weights('initial_weights.h5')\n",
        "\n",
        "history = model.fit_generator(generator=generator,\n",
        "                    epochs=300,\n",
        "                    steps_per_epoch=50,\n",
        "                    validation_data=validation_data,\n",
        "                    callbacks=[ mc, reduce_lr])\n",
        "                    #callbacks=[es, reduce_lr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm_LpGX8cUYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_df = pd.DataFrame(history.history) \n",
        "hist_csv_file = model_file.split('.')[0]+'.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hw_8bHafmhV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e37a3018-93f4-4567-eccf-d680f51af711"
      },
      "source": [
        "# Save history and H5 file to drive\n",
        "saveDrivePath = '/content/drive/My Drive/Capstone/Approach/Step4DayTrade_DSFDInput'\n",
        "h5FilePath = '/content/'+ model_file\n",
        "historyFilePath = '/content/' + hist_csv_file\n",
        "initialWeightsFilePath = '/content/initial_weights.h5'\n",
        "\n",
        "os.system(\"mv \"+'\"'+h5FilePath+'\"' + \" \" + '\"'+saveDrivePath+'\"' )\n",
        "os.system(\"mv \"+'\"'+historyFilePath+'\"' + \" \" + '\"'+saveDrivePath+'\"' )\n",
        "os.system(\"mv \"+'\"'+initialWeightsFilePath+'\"' + \" \" + '\"'+saveDrivePath+'\"' )"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZrvzgFdcUZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nngbf4FcUZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA5WOYpocUZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a3760c02-cbf1-4c4a-f065-dec1b2e390d4"
      },
      "source": [
        "# The algorithm uses data of the previous 25 time-steps to forecast the following 10th minute into the future. Therefore,  \n",
        "# the first prediction of every day can only be excecuted after 25 minutes of the begining of the day, and will refer  \n",
        "# to the prediction of the minute 35.\n",
        "\n",
        "# Since every day consists of 1440 minutes the algorithm is only able to predict the last 1405 minutes of the day.\n",
        "\n",
        "#Create a Dataframe to hold the true predicted values for each day.\n",
        "predictionsData = DayTrade.copy()\n",
        "predictionsData = predictionsData.loc[:,['Weighted_Price','date']]\n",
        "predictionsData = predictionsData[predictionsData['date'].isin(testing_days)]# Filter only the testing days\n",
        "predictionsData = predictionsData.groupby('date').apply(lambda group: group.iloc[35:])# For each day filter the last 1405 minutes (1440 - 1405 = 35)\n",
        "predictionsData.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(244470, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efSaKaIQcUZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(modelFilename):\n",
        "\n",
        "    column_name = modelFilename.split('.')[0]\n",
        "    #Step 1 - Create a column to hold predictions\n",
        "    predictionsData[column_name] = np.nan \n",
        "    \n",
        "    #Step 2 - load model for prediction   \n",
        "    loaded_model = tf.keras.models.load_model(modelFilename)\n",
        "    \n",
        "    for day in testing_days:\n",
        "        #Step 3 - Filter Input data and Target with the selected date\n",
        "        day_test_data = DayTrade_clean[DayTrade_clean['date'].values==day]\n",
        "\n",
        "        #Step 4 - Drop date column\n",
        "        day_test_data = day_test_data.drop(columns='date')\n",
        "\n",
        "        #Step 5 - Convert day data into numpy array\n",
        "        day_test_data = np.array(day_test_data)\n",
        "\n",
        "        #Step 6 - Scale data for Neural Network\n",
        "        day_test_data = x_scaler.transform(day_test_data)\n",
        "\n",
        "        #Step 7 - Reshape data to fit keras requirement to have a (x,y,z) shape\n",
        "        day_test_data = day_test_data.reshape(1,day_test_data.shape[0],day_test_data.shape[1])\n",
        "\n",
        "        #Step 8 - Reshape data into Batches using slidding window   \n",
        "        batch_val_size = day_test_data.shape[1] - sequence_length\n",
        "\n",
        "        x_val_shape = (batch_val_size, sequence_length, num_x_signal)\n",
        "        x_batch = np.zeros(shape=x_val_shape, dtype=np.float16)\n",
        "\n",
        "        for i in range(batch_val_size):\n",
        "\n",
        "            # Copy the sequences of data starting at this index.\n",
        "            x_batch[i] = day_test_data[0][i:i+sequence_length][:]\n",
        "\n",
        "        #Step 9 - Generate the prediction for that day   \n",
        "        ypred = loaded_model.predict(x_batch)\n",
        "        ypred_rescaled = y_scaler.inverse_transform(ypred)\n",
        "\n",
        "        #Step 10 - Copy the prediction values to the correspondent day in the predictionData\n",
        "        predictionsData.loc[predictionsData['date']==day,column_name] = ypred_rescaled\n",
        "    \n",
        "    return(predictionsData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjswtiLzf7Bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelPath = '/content/drive/My Drive/Capstone/Approach/Step4DayTrade_DSFDInput'\n",
        "modelfilePath = modelPath + '/' + model_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KN1qC_tcUZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(modelfilePath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlBHNrLAcUZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Dataframe with prediction as csv file\n",
        "prediction_file = 'predictions_' +  model_file.split('.')[0] + '.csv'\n",
        "prediction_filePath = '/content/' + prediction_file\n",
        "predictionsData.to_csv(prediction_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGTpSqN_gIsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "destinantionDir = '/content/drive/My Drive/Capstone/Approach/Step4DayTrade_DSFDInput/Predictions_csv'\n",
        "oscmd = \"mv \"+'\"'+prediction_filePath+'\"' + \" \" + '\"'+destinantionDir+'\"' \n",
        "oscmd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yvDgzZPgPYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move csv files to directory in Drive\n",
        "os.system(oscmd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zntWdZ5QNDmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mse_Weighted_Price = mean_squared_error(predictionsData.iloc[:,0],predictionsData.iloc[:,2])\n",
        "mse_Weighted_Price"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2yk1deqWScj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}